<< 회귀 모델 종류 >>
** 선형 모델 기반과 회귀 트리 기반으로 나뉘어짐
선형 모델 기반:
- LinearRegression (sklearn.linear_model)
- LogisticRegression (sklearn.linear_model)
- Ridge (sklearn.linear_model)
- Lasso (sklearn.linear_model)
- ElasticNet (sklearn.linear_model)

회귀 트리 기반: 
- RandomForestRegressor (sklearn.ensemble)
- GradientBoostingRegressor (sklearn.ensemble)
- DecisionTreeRegressor (sklearn.tree)
- XGBRegressor
- LGBMRegressor


======================================================

<< 회귀 모델 사용법 >>

회귀 모델 또한 분류 모델 사용 방식에서 크게 다를 것은 없음.
1. 데이터 불러오기
2. 데이터 나누기
3. 모델 생성
4. 모델 훈련 
5. 모델 평가 

** 모델 평가 부분이 분류 모델과 다름. 
   ==> 분류 모델에서는 모델 평가로 accuracy, precision, recall, f1 score, roc auc 를 사용했지만 
       회귀 모델에서의 평가 지표는 아래와 같음:
       - mse (mean squared error)
       - rmse (root mean squared error)
       - mae (mean absolute error)
       - negative rmse
       - r2 



----- [ 모델 평가 ] -----

from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

    mse=mean_squared_error(y_test, pred_model)
    rmse=sqrt(mse)
    r2=r2_score(y_test, pred_model)

    neg_mse_scores=cross_val_score(model, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
    rmse_scores=np.sqrt(-1*neg_mse_scores)
    avg_rmse=np.mean(rmse_scores)






======================================================


회귀계수 확인


======================================================





하이퍼 파라미터

XGBRegressor는 회귀 문제에 특화된 모델로, 여러 하이퍼파라미터를 통해 성능을 최적화할 수 있다.

주요 하이퍼파라미터:

n_estimators: 부스팅을 위해 생성될 결정 트리의 개수입니다. 일반적으로 더 많은 트리가 모델의 성능을 향상시킬 수 있지만, 너무 많으면 과적합(overfitting)을 일으킬 수 있습니다.

learning_rate (eta): 각 부스팅 단계에서의 스텝 사이즈로, 새로운 트리의 기여도를 조절합니다. 낮은 값은 학습을 더 안정적으로 만들지만, 너무 낮으면 많은 트리가 필요하게 되고, 이는 과적합을 초래할 수 있습니다.

max_depth: 각 트리의 최대 깊이를 제한합니다. 깊은 트리는 더 복잡한 패턴을 학습할 수 있지만, 과적합의 위험이 있습니다.

min_child_weight: 자식 노드를 분할하기 위한 인스턴스 가중치(합)의 최소값입니다. 이 값을 크게 설정하면 더 보수적인 모델이 생성됩니다.

gamma (min_split_loss): 리프 노드를 추가로 분할하기 위한 최소 손실 감소 값입니다. 큰 값은 모델이 더 보수적이 되도록 합니다.

subsample: 각 트리를 훈련할 때 사용할 데이터 샘플의 비율입니다. 이는 과적합을 방지하는 데 도움이 됩니다.

colsample_bytree/colsample_bylevel/colsample_bynode: 트리, 레벨, 노드를 기준으로 각각 훈련에 사용할 특성(열)의 비율입니다. 이를 통해 과적합을 줄일 수 있습니다.

lambda (reg_lambda): L2 정규화 항의 가중치입니다. 이는 모델의 복잡도를 제한하여 과적합을 방지하는 데 도움이 됩니다.

alpha (reg_alpha): L1 정규화 항의 가중치입니다. 불필요한 특성의 가중치를 0으로 만들어 모델을 더 단순하게 만듭니다.

scale_pos_weight: 불균형 데이터셋을 다룰 때 양성 클래스의 가중치를 조절합니다. 주로 분류 문제에서 사용됩니다.

objective: 최적화할 손실 함수를 지정합니다. 회귀 문제에서는 reg:squarederror, reg:linear, reg:gamma 등이 사용될 수 있습니다.

random_state: 결과의 재현 가능성을 위한 난수 시드입니다.



LightGBM의 여러 하이퍼파라미터를 통해 성능을 조정할 수 있다.

주요 하이퍼파라미터:

n_estimators: 부스팅을 위해 생성될 결정 트리의 개수입니다. 더 많은 트리는 일반적으로 모델의 성능을 향상시킬 수 있으나, 너무 많으면 과적합을 일으킬 수 있습니다.

learning_rate: 각 부스팅 단계에서의 업데이트 크기를 조절합니다. 낮은 학습률은 학습 과정을 더 안정적으로 만들 수 있지만, 충분한 성능을 달성하기 위해 더 많은 트리가 필요할 수 있습니다.

max_depth: 트리의 최대 깊이를 제한합니다. 깊은 트리는 더 복잡한 패턴을 학습할 수 있지만, 과적합의 위험을 증가시킵니다.

num_leaves: 하나의 트리가 가질 수 있는 최대 리프의 수입니다. LightGBM에서는 이 값을 통해 트리의 복잡성을 조절합니다. num_leaves의 값이 크면 모델의 복잡도가 증가하므로, 과적합에 주의해야 합니다.

min_child_samples (min_data_in_leaf): 리프 노드가 가지고 있어야 할 최소 데이터 수입니다. 과적합을 방지하기 위해 사용됩니다.

min_child_weight: 자식 분할을 가지기 위한 인스턴스 가중치 합의 최소값입니다. 더 큰 값은 모델을 더 보수적으로 만듭니다.

subsample (bagging_fraction): 트리를 구축할 때 사용할 데이터 샘플의 비율입니다. 이는 과적합을 방지하는 데 도움이 됩니다.

colsample_bytree (feature_fraction): 트리를 구축할 때 사용할 특성(열)의 비율입니다. 이를 통해 과적합을 방지하고 학습 속도를 향상시킬 수 있습니다.

reg_alpha (lambda_l1): L1 정규화 항의 가중치입니다. 이는 모델의 복잡도를 줄이는 데 도움이 됩니다.

reg_lambda (lambda_l2): L2 정규화 항의 가중치입니다. 이는 또한 모델의 복잡도를 줄이고 과적합을 방지하는 데 도움이 됩니다.

max_bin: 수치형 변수를 이산화(discretization)할 때 사용되는 최대 bin의 수입니다. 이 값이 크면 모델의 정확도는 향상될 수 있지만, 과적합의 위험과 메모리 사용량이 증가합니다.

boosting_type: 부스팅 타입을 지정합니다. LightGBM은 gbdt (기본값), rf (랜덤포레스트), dart, goss 등 여러 부스팅 타입을 지원합니다.

objective: 최적화할 손실 함수를 지정합니다. 회귀 문제의 경우 regression, regression_l1, `huber




======================================================

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# 캘리포니아 주택 데이터셋 불러오기
housing = fetch_california_housing()
housing.data.shape
X = housing.data
y = housing.target
# 데이터셋을 데이터프레임으로
df = pd.DataFrame(X, columns=housing.feature_names)
df['Target'] = y
df.head()

X=df.drop('Target', axis=1)
y=df['Target']

# 타겟 변수에 로그 변환 적용
y_log=np.log1p(y)

# 특성 스케일링
scaler=StandardScaler()
X_scaled=scaler.fit_transform(X)

# 훈련 데이터와 테스트 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_log, test_size=0.2, random_state=0)

# 선형 회귀 모델 생성 및 훈련
lin_reg=LinearRegression()
lin_reg.fit(X_train, y_train)

# 테스트 데이터에 대한 예측 및 성능 평가
y_pred=lin_reg.predict(X_test)
mse=mean_squared_error(y_test, y_pred)
rmse=np.sqrt(mse) # Calculate RMSE
r2=r2_score(y_test, y_pred) # Calculate R2 score

print("계수: ", lin_reg.coef_)
print("절편: ", lin_reg.intercept_)
print("평균 제곱 오차(MSE): ", mse)
print("Root Mean Square Error (RMSE): ", round(rmse,2))
print("R2 Score: ", round(r2,2))
