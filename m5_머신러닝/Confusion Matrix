<< Confusion Matrix >>
- 기계학습에서 모델이나 패턴의 분류 성능을 평가할 때 사용되는 지표
- 예측하고자 하는 대상 A에 대해 제대로 예측한 케이스와 예측하지 못한 케이스를 세어보는 방식임
- 모델 평가는 예측과 정답으로 구분될 수 있는데, 각각 아래처럼 구분됨: 
    - 예측은 양성 (Positive)와 음성 (Negative)
    - 정답은 정답 (True)와 오답 (False)
      ==> 따라서 총 4가지 조합이 나올 수 있음: 
          1. 정답, 예측 양성 (TP) : 양성으로 예측했으며 실제로도 양성이 맞음 (정답)
          2. 오답, 예측 양성 (FP) : 양성으로 예측했으나 실제로는 음성임 (오답)
          3. 정답, 예측 음성 (FN) : 음성으로 예측했으나 실제로는 양성임 (오답)
          4. 오답, 예측 음성 (TN) : 음성으로 예측했으며 실제로도 음성이 맞음 (정답)

- 위와 같이 분리한 4가지의 케이스로 분류 모델의 성능을 다음과 같이 평가해볼 수 있음:
  1. 정확도 (Accuracy)
  2. 정밀도 (Precision)
  3. 재현율 (Recall) / 민감도 (Sensitivity)
  4. F1 Score 
  5. ROC AUC (Receiver Operating Characteristics - Area Under Curve)

정확도 : 모델의 전체 데이터 중 양성/음성에 대한 예측 정답율
        = (TP+TN)/(TP+FP+FN+TN)

정밀도 : 양성으로 예측된 데이터 중 실제로 양성인 데이터의 비율
        = TP/(TP+FP)
          ==> 즉, 양성이라고 예측했는데 실제 양성인 케이스 (TP)와 양성으로 예측했지만 음성(FP)이었던 케이스 중에서 실제로 양성으로 예측을 제대로 (TP) 한 비율임
          ** 정밀도가 중요할 때는: 
             실제 Negative인데 예측이 Positive로 잘못 판단되었을 때의 영향이 큰 경우에 중요
             e.g. 스팸 메일 여부 판단 등 (실제로 스팸이 아닌데 (negative) 스팸으로 (positive) 잘못 분류하여 중요한 메일을 받지 못할 경우)

재현율 : 실제 양성인 데이터 중 모델이 양성으로 제대로 예측한 데이터의 비율
        = TP/(TP+FN)
          ==> 즉, 양성인데 양성으로 제대로 예측한 케이스 (TP)와 양성인데 음성으로 잘못 예측한 케이스(FN)이었던 케이스 중에서 실제로 양성으로 제대로 예측한 (TP) 비율  
          ** 재현율이 중요할 때는:
             실제 Positive 데이터를 Negative로 잘못 판단하게 되었을 때의 영향이 큰 경우에 중요
             e.g. 질병 진단 등 (실제로는 암인데 암이 아니라고 판정할 경우 환자의 생명에 위협이 커짐)
          ***** 실제 업무에서는 재현율을 더 중요하게 볼 때가 많음. 실제로 문제가 있는데 문제가 없다고 예상함으로써 더 큰 위험을 감수해야할 수 있으므로.


F1 score : 정밀도와 재현율의 조화 평균 (harmonic mean).
           정밀도와 재현율을 결합한 지표이며, 정밀도와 재현율이 어느 한쪽에도 치우치지 않는 수치일 때 높게 나타남 
           = 2* (정밀도*재현율)/(정밀도+재현율)

           정밀도와 재현율을 trade-off 관계에 있음. 
           정밀도가 높으면 재현율은 낮으며, 재현율이 높으면 정밀도가 낮아지게 됨.
             >> 정밀도는 높은데 재현율이 낮다면 해당 모델은 긍정 예측이 매우 신중하지만 많은 실제 긍정을 놓치고 있음 (재현율이 낮다는 것은 실제 양성인데 음성으로 판단한 케이스가 높다는 뜻...?))
             >> 재현율은 높은데 정밀도가 낮다면 해당 모델은 양성을 잘 찾아내지만 동시에 오탐 또한 많이 포함하고 있다는 뜻 

           따라서, 분류 기준이 될 임곗치 (threshold) 설정으로 정밀도/재현율을 어떻게 조절해 볼것이냐는 문제 분류의 중요성을 어느쪽에 더 무게를 둘 것이냐에 따라 달라짐
           ==> threshold는 양성 예측값을 결정하는 확률의 기준 (0~1).
               threshold가 낮아진 진다는 것은 양성 예측 기준을 더 너그럽게 풀어준다는 것. 
               >> 즉, 양성 분류 기준이 너그러워져서 양성 예측 확률이 올라감 (FP 증가, FN 감소)
                  ** 따라서, threshold를 낮추면 재현율이 상승, 정밀도는 감소함 



ROC : 여러 임계값들을 (threshold)를 기준으로 recall-fallout의 변화를 시각화한 것. 
      ==> Fallout은 실제 False인 데이터 중에서 모델이 True로 분류한,
          Recall은 실제 True인 데이터 중에서 모델이 True로 분류한 비율을 나타내는 지표.
      ==> x축을 False Positive Rate (1-specificity)
          y축을 True Positive Rate (재현율) 을 놓고 그리는 그래프임
          ***** curve가 왼쪽 위 모소리에 가까울 수록 모델의 성능이 좋음 *****

AUC : ROC curve는 그래프이기 때문에 명확한 수치로 비교가 어려움. 따라서 그래프 아래의 면적값을 이용.
      ==> 최대값은 1이며, 1에 가까울 수록 좋은 모델 (즉, fall-out 대비 recall 값이 클수록 좋음)



----- [ 정확성 ] -----





----- [ 정밀성 ] -----





----- [ 재현율/민감도 ] -----




----- [ F1 score ] -----





----- [ ROC AUC ] -----

