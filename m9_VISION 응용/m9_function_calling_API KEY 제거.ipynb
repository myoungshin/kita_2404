{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNk8695WITfX31C8AWHIFE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://platform.openai.com/docs/guides/function-calling\n","\n","--> Open AI에서 설명하는 function calling에 대한 url"],"metadata":{"id":"2wJCo7j1AkfU"}},{"cell_type":"markdown","source":["#### Function Calling의 사용\n","- 이 코드에서 function calling은 모델이 적절한 시점에 외부 함수(get_current_weather)를 호출하도록 유도합니다. 모델이 What is the current weather in {user_location}?와 같은 질문에 응답할 때, 해당 함수가 정의되어 있으면 모델은 그 함수를 호출하여 날씨 정보를 가져오려 합니다.\n","\n","- function_call=\"auto\" 설정을 통해 GPT 모델은 적절한 함수가 있을 때 자동으로 그 함수를 호출할 수 있습니다. 여기서는 get_current_weather 함수 명세를 통해 사용자가 입력한 위치에 맞는 날씨 정보를 가져올 수 있도록 유도됩니다.\n","\n","- 만약 모델이 함수 호출을 결정하면, 응답에 function_call이 포함되며, 이때 함수 이름과 그 인자가 함께 반환됩니다. 그런 후에 함수가 호출되고, 결과가 다시 모델에 제공됩니다.\n","\n","이 과정에서 모델은 단순히 인자를 생성하고, 함수를 호출하는 것은 개발자의 책임입니다. OpenAI의 API는 실제 함수 실행을 수행하지 않으므로, 함수 호출 후 결과를 처리하고 다시 모델에 넘기는 과정을 수동으로 처리하게 됩니다."],"metadata":{"id":"Nzz2vJapAltC"}},{"cell_type":"code","source":["!pip install openai --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5c1KuAijB7me","executionInfo":{"status":"ok","timestamp":1724027242423,"user_tz":-540,"elapsed":7242,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"99b32b42-845f-4cb4-876b-2bd1674644f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/362.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/362.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/362.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m358.4/362.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.4/362.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["Open Weather 에서 API key 받아 실시간 날씨 확인\n","\n","https://home.openweathermap.org/"],"metadata":{"id":"_XTHZkUYHEv8"}},{"cell_type":"code","source":["import openai\n","import json\n","import requests\n","\n","# OpenAI API 키 설정\n","from openai import OpenAI\n","client=OpenAI(api_key='')\n","\n","# 날쌔ㅣ API 키 설정 (예: OpenWeatherMap)\n","weather_api_key=''\n","\n","# 날씨 정보를 가져오는 함수 정의\n","def get_current_weather(location):\n","    url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={weather_api_key}&units=metric\"\n","    response=requests.get(url)\n","    return response.json()\n","\n","# 사용자 요청 메시지\n","messages=[\n","    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","    {\"role\": \"user\", \"content\" : \"What is the current weather in New York?\"}\n","]\n","\n","# GPT-4o-mini 모델 호출\n","response=client.chat.completions.create(\n","    model='gpt-4o-mini-2024-07-18',\n","    messages=messages,\n","    functions=[{\n","        \"name\": \"get_current_weather\",\n","        \"description\": \"Get the current weather for a specific location\",\n","        \"parameters\": {\"type\": \"object\",\n","                       \"properties\": {\"location\": {\"type\": \"string\",\"description\": \"The name of the city to get the weather for\"}\n","            },\n","            \"required\": [\"location\"]\n","        }\n","    }],\n","    function_call=\"auto\"\n",")\n","\n","# 모델의 응답 메시지를 messages 리스트에 추가하고 출력합니다\n","response_message=response.choices[0].message\n","messages.append(response_message)\n","\n","print(response_message)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ya-b7A7IDgxP","executionInfo":{"status":"ok","timestamp":1724028498452,"user_tz":-540,"elapsed":834,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"a260eb97-435d-48a8-db9c-cb4bad97df2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"New York\"}', name='get_current_weather'), tool_calls=None)\n"]}]},{"cell_type":"code","source":["# (위 셀과 이어서 작성됨)\n","# 도구 호출 여부 확인\n","function_call=response_message.function_call\n","if function_call:\n","    tool_function_name=function_call.name\n","    tool_arguments = json.loads(function_call.arguments)\n","\n","    # 함수 호출 및 결과 처리\n","    if tool_function_name=='get_current_weather':\n","        location=tool_arguments['location']\n","        weather_results=get_current_weather(location)\n","\n","        # 함수 호출 결과 메세지 추가\n","        messages.append({\n","            \"role\": \"function\",\n","            \"name\": tool_function_name,\n","            \"content\": json.dumps(weather_results) # JSON 형식으로 반환\n","        })\n","\n","        # 모델 재호출\n","        model_response_with_function_call=client.chat.completions.create(\n","            model=\"gpt-4o-mini-2024-07-18\",\n","            messages=messages\n","        )\n","        print(model_response_with_function_call.choices[0].message.content)\n","    else:\n","        print(f\"Error: function {tool_function_name} does not exist\")\n","else:\n","    # 도구 호출이 없는 경우 결과 반환\n","    print(response_message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nugQwGBDLP9z","executionInfo":{"status":"ok","timestamp":1724029983394,"user_tz":-540,"elapsed":1620,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"c4f5f50c-727d-4ccb-8190-604b0e9a2a26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The current weather in New York is as follows:\n","\n","- **Temperature:** 21.6°C (feels like 22.17°C)\n","- **Conditions:** Mist with moderate rain\n","- **Humidity:** 90%\n","- **Wind:** Speed of 4.63 m/s from the northeast, with gusts up to 7.72 m/s\n","- **Visibility:** 4828 meters\n","- **Rainfall:** 1.65 mm in the last hour\n","- **Cloud Cover:** 100%\n","\n","Overall, it's a misty and rainy day in New York.\n"]}]},{"cell_type":"markdown","source":["사용자에게 날씨 확인하고 싶은 도시 이름은 받아서 함수 돌리기"],"metadata":{"id":"IT4N7hlBN7UY"}},{"cell_type":"code","source":["# 1. 사용자 입력 받아 messages.append로 추가"],"metadata":{"id":"3oyZs4oFUnre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openai\n","import json\n","import requests\n","\n","# OpenAI API 키 설정\n","from openai import OpenAI\n","client=OpenAI(api_key='')\n","\n","# 날쌔ㅣ API 키 설정 (예: OpenWeatherMap)\n","weather_api_key=''\n","\n","# 날씨 정보를 가져오는 함수 정의\n","def get_current_weather(location):\n","    url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={weather_api_key}&units=metric\"\n","    response=requests.get(url)\n","    return response.json()\n","\n","# 사용자 요청 메시지 초기화\n","messages=[\n","    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","    {\"role\": \"user\", \"content\" : \"Please provide the location to get the current weather information.\"}\n","]\n","\n","# 사용자 입력 받기\n","user_location=input(str(\"Enter the location to get the current weather: \"))\n","messages.append({\"role\": \"user\", \"content\": f\"What is the current weather in {user_location}\"}) # 사용자 입력 받은 내용을 messages 에 추가해주기\n","\n","# GPT-4o-mini 모델 호출\n","response=client.chat.completions.create(\n","    model='gpt-4o-mini-2024-07-18',\n","    messages=messages,\n","    functions=[{\n","        \"name\": \"get_current_weather\",\n","        \"description\": \"Get the current weather for a specific location\",\n","        \"parameters\": {\"type\": \"object\",\n","                       \"properties\": {\"location\": {\"type\": \"string\",\"description\": \"The name of the city to get the weather for\"}\n","            },\n","            \"required\": [\"location\"]\n","        }\n","    }],\n","    function_call=\"auto\"\n",")\n","\n","# 모델의 응답 메시지를 messages 리스트에 추가하고 출력합니다\n","response_message=response.choices[0].message\n","messages.append(response_message)\n","\n","print(response_message)\n","\n","\n","# (위 셀과 이어서 작성됨)\n","# 도구 호출 여부 확인\n","function_call=response_message.function_call\n","if function_call:\n","    tool_function_name=function_call.name\n","    tool_arguments = json.loads(function_call.arguments)\n","\n","    # 함수 호출 및 결과 처리\n","    if tool_function_name=='get_current_weather':\n","        location=tool_arguments['location']\n","        weather_results=get_current_weather(location)\n","\n","        # 함수 호출 결과 메세지 추가\n","        messages.append({\n","            \"role\": \"function\",\n","            \"name\": tool_function_name,\n","            \"content\": json.dumps(weather_results) # JSON 형식으로 반환\n","        })\n","\n","        # 모델 재호출\n","        model_response_with_function_call=client.chat.completions.create(\n","            model=\"gpt-4o-mini-2024-07-18\",\n","            messages=messages\n","        )\n","        print(model_response_with_function_call.choices[0].message.content)\n","    else:\n","        print(f\"Error: function {tool_function_name} does not exist\")\n","else:\n","    # 도구 호출이 없는 경우 결과 반환\n","    print(response_message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gE00ReO1N1Mw","executionInfo":{"status":"ok","timestamp":1724031461452,"user_tz":-540,"elapsed":6036,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"8152eb87-c3f4-44e7-a2ea-7b53c2090e08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the location to get the current weather: New York\n","ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"New York\"}', name='get_current_weather'), tool_calls=None)\n","The current weather in New York is as follows:\n","\n","- **Condition:** Broken clouds\n","- **Temperature:** 21.73°C\n","- **Feels Like:** 22.36°C\n","- **Humidity:** 92%\n","- **Wind Speed:** 4.12 m/s (from the northeast)\n","- **Cloud Cover:** 75%\n","- **Pressure:** 1008 hPa\n","\n","If you need more specific details or forecasts, feel free to ask!\n"]}]},{"cell_type":"code","source":["# 2. 사용자 입력을 messages에서 바로 받아 쓰기"],"metadata":{"id":"hWeLy1nNUrZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openai\n","import json\n","import requests\n","\n","# OpenAI API 키 설정\n","from openai import OpenAI\n","client=OpenAI(api_key='')\n","\n","# 날쌔ㅣ API 키 설정 (예: OpenWeatherMap)\n","weather_api_key=''\n","\n","# 날씨 정보를 가져오는 함수 정의\n","def get_current_weather(location):\n","    url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={weather_api_key}&units=metric\"\n","    response=requests.get(url)\n","    return response.json()\n","\n","# 사용자 입력 받기\n","user_location=input(str(\"Enter the location to get the current weather: \"))\n","\n","# 사용자 요청 메시지 초기화\n","messages=[\n","    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","    {\"role\": \"user\", \"content\" : f\"What is the current weather in {user_location}\"}\n","]\n","\n","# GPT-4o-mini 모델 호출\n","response=client.chat.completions.create(\n","    model='gpt-4o-mini-2024-07-18',\n","    messages=messages,\n","    functions=[{\n","        \"name\": \"get_current_weather\",\n","        \"description\": \"Get the current weather for a specific location\",\n","        \"parameters\": {\"type\": \"object\",\n","                       \"properties\": {\"location\": {\"type\": \"string\",\"description\": \"The name of the city to get the weather for\"}\n","            },\n","            \"required\": [\"location\"]\n","        }\n","    }],\n","    function_call=\"auto\"\n",")\n","\n","# 모델의 응답 메시지를 messages 리스트에 추가하고 출력합니다\n","response_message=response.choices[0].message\n","messages.append(response_message)\n","\n","print(response_message)\n","\n","\n","# (위 셀과 이어서 작성됨)\n","# 도구 호출 여부 확인\n","function_call=response_message.function_call\n","if function_call:\n","    tool_function_name=function_call.name\n","    tool_arguments = json.loads(function_call.arguments)\n","\n","    # 함수 호출 및 결과 처리\n","    if tool_function_name=='get_current_weather':\n","        location=tool_arguments['location']\n","        weather_results=get_current_weather(location)\n","\n","        # 함수 호출 결과 메세지 추가\n","        messages.append({\n","            \"role\": \"function\",\n","            \"name\": tool_function_name,\n","            \"content\": json.dumps(weather_results) # JSON 형식으로 반환\n","        })\n","\n","        # 모델 재호출\n","        model_response_with_function_call=client.chat.completions.create(\n","            model=\"gpt-4o-mini-2024-07-18\",\n","            messages=messages\n","        )\n","        print(model_response_with_function_call.choices[0].message.content)\n","    else:\n","        print(f\"Error: function {tool_function_name} does not exist\")\n","else:\n","    # 도구 호출이 없는 경우 결과 반환\n","    print(response_message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcYFYhJCUSrI","executionInfo":{"status":"ok","timestamp":1724032211370,"user_tz":-540,"elapsed":6926,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"884b86dc-2e00-4eef-9e0e-21ed52797a3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the location to get the current weather: Sydney\n","ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Sydney\"}', name='get_current_weather'), tool_calls=None)\n","The current weather in Sydney is as follows:\n","\n","- **Condition**: Overcast clouds\n","- **Temperature**: 14.85°C (feels like 14.48°C)\n","- **Humidity**: 80%\n","- **Wind Speed**: 0.89 m/s from the east\n","- **Visibility**: 10,000 meters\n","- **Cloud Coverage**: 100%\n","\n","Overall, the weather is quite cloudy and cool.\n"]}]},{"cell_type":"markdown","source":["## How to call functions with model generated arguments\n","\n","다음 예제에서는 모델에서 생성된 입력을 갖는 함수를 실행하는 방법을 보여주고 이를 사용하여 데이터베이스에 대한 질문에 답할 수 있는 에이전트를 구현합니다. 단순화를 위해 Chinook 샘플 데이터베이스를 사용하겠습니다 .\n","\n","참고: 모델이 올바른 SQL을 생성하는 데 완벽하게 신뢰할 수 없기 때문에 프로덕션 환경에서 SQL 생성은 위험할 수 있습니다.\n","\n","이 코드는 OpenAI의 GPT 모델을 사용하여 SQLite 데이터베이스에서 음악 관련 질문에 대한 답변을 SQL 쿼리로 변환한 후, 해당 쿼리를 실행하여 결과를 반환하는 방식으로 function calling을 구현한 예시입니다. 여기서는 사용자가 앨범 관련 질문을 하면, GPT-4가 질문을 SQL 쿼리로 변환하고, SQLite 데이터베이스를 조회하여 결과를 반환합니다."],"metadata":{"id":"CeIUfNs3VN4P"}},{"cell_type":"markdown","source":["Specifying a function to execute SQL queries"],"metadata":{"id":"a1EneGzXZgUe"}},{"cell_type":"code","source":["import sqlite3\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","# sqlite3.connect()를 통해 Google Drive 에 있는 Chinnok.db SQLite 데이터베이스에 연결\n","conn=sqlite3.connect(\"/content/drive/MyDrive/KDT_240424/m9_LLM/data/Chinook.db\")\n","print(\"Opened database successfully\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpLXyH22YlH6","executionInfo":{"status":"ok","timestamp":1724033270079,"user_tz":-540,"elapsed":2873,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"311b060d-193b-49ae-a325-b0d0573bf55a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Opened database successfully\n"]}]},{"cell_type":"markdown","source":["데이터베이스 테이블 및 열 정보 조회\n","\n","- get_table_names, get_column_names 함수를 통해 데이터베이스의 테이블 및 열 이름을 가져옵니다.\n","- 이 정보는 나중에 GPT 모델이 SQL 쿼리를 생성할 때 사용할 스키마 정보를 제공하는 데 활용됩니다."],"metadata":{"id":"RWHTDS7lZBSg"}},{"cell_type":"code","source":["# 데이터베이스에서 테이블 목록을 추출하는 함수.\n","# sqlite_master 테이블에서 'type'이 ' table'인 항목들의 이름을 가져와\n","def get_table_names(conn):\n","    \"\"\"Return a list of table names.\"\"\"\n","    table_names=[]\n","    tables=conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\") # <== execute 괄호 안에 들었는 것은 SQL 쿼리문임\n","    for table in tables.fetchall():\n","        table_names.append(table[0])\n","    return table_names\n","\n","# PRAGMA table_info(table_names) 명령을 사용하여 테이블의 스키마 정보를 가져오고, 컬럼 이름을 리스트로 반환\n","def get_column_names(conn, table_name):\n","    \"\"\"Return a list of column names.\"\"\"\n","    column_names=[]\n","    columns=conn.execute(f\"PRAGMA table_info('{table_name}');\").fetchall()\n","    for col in columns:\n","        column_names.append(col[1])\n","    return column_names\n","\n","def get_database_info(conn):\n","    \"\"\"Return a list of dicts containing the table name and columns for each table in the database.\"\"\"\n","    table_dicts=[]\n","    for table_name in get_table_names(conn):\n","        columns_names=get_column_names(conn, table_name)\n","        table_dicts.append({\"table_name\": table_name, \"column_names\": columns_names})\n","    return table_dicts"],"metadata":{"id":"7qpu2YEPZs9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터베이스의 테이블과 열 정보를 문자열 형태로 저장하여, 나중에 SQL 쿼리 작성 시 참조할 수 있도록 함"],"metadata":{"id":"G8SoGYI8caTa"}},{"cell_type":"code","source":["database_schema_dict=get_database_info(conn)\n","database_schema_string=\"\\n\".join([\n","    f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n","    for table in database_schema_dict\n","])"],"metadata":{"id":"0cD4oN1hcZz9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tools라는 리스트에 ask_database라는 함수 명세를 정의합니다.\n","- 이 함수는 사용자의 질문에 맞는 SQL 쿼리를 받아 데이터베이스에서 정보를 조회하고, 이를 반환하는 기능을 합니다.\n","- 함수의 매개변수로 query가 있으며, 이는 SQL 쿼리를 텍스트 형태로 전달받아 실행하는 구조입니다.\n","- 중요한 점은 database_schema_string이 함수 설명에 포함되어 있어, 모델이 데이터베이스 스키마에 맞는 SQL 쿼리를 생성할 수 있도록 도움을 줍니다."],"metadata":{"id":"M-B16Vklc742"}},{"cell_type":"code","source":["tools=[\n","    {\n","        \"type\": \"function\",\n","        \"function\": {\"name\": \"ask_database\",\n","        \"description\": \"Use this function to answer user questions about music. Input should be a fully formed SQL query.\",\n","        \"parameters\": {\"type\": \"object\", \"properties\":{\"query\":\n","                                                       {\"type\": \"string\",\n","                                                        \"description\": f\"\"\"SQL query extrating info to answer the user's question.\n","                                                                           SQL should be written using this database schema:\n","                                                                           {database_schema_string}\n","                                                                           The query should be returned in plain text, not in JSON.\"\"\",}\n","                                                       },\n","                                                        \"required\": [\"query\"],\n","                       },\n","                    }\n","    }\n","]"],"metadata":{"id":"kWXrwJ7Ydcig"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Executing SQL queries\n","\n","이제 실제로 데이터베이스에 대한 쿼리를 실행하는 함수를 구현해 보겠습니다."],"metadata":{"id":"XXWr0Ptof995"}},{"cell_type":"code","source":["def ask_database(conn, query):\n","    \"\"\"Function to query SQLite database with a provided SQL query.\"\"\"\n","    try:\n","        results=str(conn.execute(query).fetchall())\n","    except Exception as e:\n","        results=f\"query failed with error: {e}\"\n","    return results"],"metadata":{"id":"niBz4J78eevE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Steps to invoke a function call using Chat Completions API:\n","\n","- 1단계 : 모델이 사용할 도구를 선택하도록 하는 내용으로 모델을 프롬프트합니다. 함수 이름 및 서명과 같은 도구에 대한 설명은 '도구' 목록에 정의되어 API 호출에서 모델에 전달됩니다. 선택한 경우 함수 이름과 매개변수가 응답에 포함됩니다.\n","- 2단계 : 모델이 함수를 호출하려고 하는지 프로그래밍적으로 확인합니다. 참이면 3단계로 진행합니다.\n","- 3단계 : 응답에서 함수 이름과 매개변수를 추출하고 매개변수와 함께 함수를 호출합니다. 결과를 메시지에 추가합니다.\n","- 4단계 : 메시지 목록으로 채팅 완료 API를 호출하여 응답을 가져옵니다."],"metadata":{"id":"fxPLT4NhgYZm"}},{"cell_type":"code","source":["# 사용자의 요청 메세지를 정의합니다\n","messages=[{\n","    \"role\": \"user\", \"content\": \"What is the name of the album with the most tracks?\"\n","}]\n","\n","# 사용자의 질문에 대한 응답을 생성. tools와 tool_choice 파라미터는 모델이 데이터베이스 쿼리를 인식하고 자동으로 도구를 선택할 수 있도록 돕습니다.\n","response=client.chat.completions.create(\n","    model=\"gpt-4o-mini-2024-07-18\",\n","    messages=messages,\n","    tools=tools,\n","    tool_choice=\"auto\" # 모델이 자동으로 함수 호출을 결정, SQL 쿼리로 변환할 필요가 있다고 판단하면 자동으로 ask_database 함수를 호출하게 됩니다.\n",")\n","\n","# 모델의 응답 메시지를 messages 리스트에 추가하고 출력합니다\n","response_message=response.choices[0].message\n","messages.append(response_message)\n","\n","print(response_message)\n","\n","# 여기서 tool_calls, function_call 를 지정하지 않았음에도 하기에 포함되어 나오는 이유는 여기서 사용한 모델이 최신 모델이기 때문.\n","# 위에서 tools라 넣은 이유는 gpt-4o (예전 모델)을 사용했기 때문에 설정해줘야 했던 것."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtE8o9dUhEIH","executionInfo":{"status":"ok","timestamp":1724035651892,"user_tz":-540,"elapsed":1250,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"a4b17dc8-5142-408c-94d0-33b3caad6a12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_H0utJc8qg0TIigXhgT15ig3l', function=Function(arguments='{\"query\":\"SELECT Album.Title, COUNT(Track.TrackId) AS TrackCount FROM Album JOIN Track ON Album.AlbumId = Track.AlbumId GROUP BY Album.AlbumId ORDER BY TrackCount DESC LIMIT 1;\"}', name='ask_database'), type='function')])\n"]}]},{"cell_type":"code","source":["# 모델 응답에서 도구 호출이 포함되어 있는지 확인하고, 도구 호출이 있다면 도구 호출  ID, 함수 이름 및 쿼리 문자열을 추출합니다\n","tool_calls=response_message.tool_calls\n","if tool_calls:\n","    # if true the model will return the name of the tool/ function to call and the argument(s)\n","    tool_call_id=tool_calls[0].id\n","    tool_function_name=tool_calls[0].function.name\n","    tool_query_string=json.loads(tool_calls[0].function.arguments)['query']\n","\n","    # 도구 호출 함수 이름이 'ask_database'인 경우, ask_database 함수를 호출하여 데이터베이스 쿼리를 실행하고 결과를 messages 리스트에 추가합니다\n","    if tool_function_name=='ask_database':\n","        results=ask_database(conn, tool_query_string)\n","\n","        messages.append({\n","            \"role\": \"tool\",\n","            \"tool_call_id\": tool_call_id,\n","            \"name\": tool_function_name,\n","            \"content\": results\n","        })\n","\n","        # 도구 호출 결과가 포함된 messages 리스트를 사용하여 모델을 다시 호출하고 최종 응답을 출력합니다.\n","        # Note that messages with role 'tool' must be a response to a preceding message with 'tool_calls'\n","        model_response_with_function_call=client.chat.completions.create(\n","            model=\"gpt-4o-mini-2024-07-18\",\n","            messages=messages,\n","        ) # get a new response from the model where it can see the function response\n","        print(model_response_with_function_call.choices[0].message.content)\n","        # 도구 호출 함수 이름이 'ask_database'가 아닌 경우 오류 메세지를 출력하거나, 도구 호출이 없으면 모델의 응답 내용을 바로 출력합니다.\n","    else:\n","        print(f\"Error: function {tool_function_name} does not exist\")\n","else:\n","    # Model did not identify a function to call, result can be returned to the user\n","    print(response_message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r3H72nQojFAW","executionInfo":{"status":"ok","timestamp":1724036639875,"user_tz":-540,"elapsed":870,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"f1457f53-42ba-4202-caaa-7b73455bba3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The album with the most tracks is titled \"Greatest Hits,\" which contains a total of 57 tracks.\n"]}]},{"cell_type":"code","source":["import sqlite3\n","import json\n","import openai\n","import sqlite3\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# OpenAI API 키 설정\n","from openai import OpenAI\n","client = OpenAI(api_key=\"\")\n","\n","# 데이터베이스 연결\n","conn = sqlite3.connect(\"/content/drive/MyDrive/KDT_240424/m9_LLM/data/Chinook.db\")\n","print(\"Opened database successfully\")\n","\n","# 함수 정의\n","def get_table_names(conn):\n","    table_names = []\n","    tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n","    for table in tables.fetchall():\n","        table_names.append(table[0])\n","    return table_names\n","\n","def get_column_names(conn, table_name):\n","    column_names = []\n","    columns = conn.execute(f\"PRAGMA table_info('{table_name}');\").fetchall()\n","    for col in columns:\n","        column_names.append(col[1])\n","    return column_names\n","\n","def get_database_info(conn):\n","    table_dicts = []\n","    for table_name in get_table_names(conn):\n","        columns_names = get_column_names(conn, table_name)\n","        table_dicts.append({\"table_name\": table_name, \"column_names\": columns_names})\n","    return table_dicts\n","\n","database_schema_dict = get_database_info(conn)\n","database_schema_string = \"\\n\".join(\n","    [\n","        f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n","        for table in database_schema_dict\n","    ]\n",")\n","\n","# 함수 정의\n","def ask_database(conn, query):\n","    \"\"\"Function to query SQLite database with a provided SQL query.\"\"\"\n","    try:\n","        results = str(conn.execute(query).fetchall())\n","    except Exception as e:\n","        results = f\"query failed with error: {e}\"\n","    return results\n","\n","# 사용자의 질문에 대한 응답을 생성하는 함수\n","messages = [{\n","    \"role\":\"user\",\n","    \"content\": \"What is the name of the album with the most tracks?\"\n","}]\n","\n","# 'functions' 정의\n","functions = [\n","    {\n","        \"name\": \"ask_database\",\n","        \"description\": \"Use this function to answer user questions about music. Input should be a fully formed SQL query.\",\n","        \"parameters\": {\n","            \"type\": \"object\",\n","            \"properties\": {\n","                \"query\": {\n","                    \"type\": \"string\",\n","                    \"description\": f\"\"\"\n","                        SQL query extracting info to answer the user's question.\n","                        SQL should be written using this database schema:\n","                        {database_schema_string}\n","                        The query should be returned in plain text, not in JSON.\n","                    \"\"\"\n","                }\n","            },\n","            \"required\": [\"query\"]\n","        }\n","    }\n","]\n","\n","# 모델 응답 요청\n","response = client.chat.completions.create(\n","    model=\"gpt-4o-mini-2024-07-18\",\n","    messages=messages,\n","    functions=functions,  # 'tools' 대신 'functions' 사용\n","    function_call=\"auto\"  # 'tool_choice' 대신 'function_call' 사용\n",")\n","\n","# 모델의 응답 메시지와 함수 호출 처리\n","response_message = response.choices[0].message\n","messages.append(response_message)\n","\n","\n","# function_call이 있는지 확인\n","if response_message.function_call:  # 'function_call' 속성을 직접 확인\n","    # 함수 호출 정보 추출\n","    function_name = response_message.function_call.name\n","    function_arguments = json.loads(response_message.function_call.arguments)\n","\n","    # 'ask_database' 함수 호출\n","    if function_name == 'ask_database':\n","        query = function_arguments['query']\n","        results = ask_database(conn, query)\n","\n","        messages.append({\n","            \"role\": \"function\",\n","            \"name\": function_name,\n","            \"content\": results\n","        })\n","\n","        # 모델의 최종 응답 요청\n","        final_response = client.chat.completions.create(\n","            model=\"gpt-4o-mini-2024-07-18\",\n","            messages=messages,\n","        )\n","\n","        print(final_response.choices[0].message.content)\n","else:\n","    # 함수 호출이 없으면 결과 출력\n","    print(response_message.content)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjPqpnlWPqk_","executionInfo":{"status":"ok","timestamp":1724114733080,"user_tz":-540,"elapsed":21229,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"e33f71f4-d9ab-438a-de25-37d4737d9b16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Opened database successfully\n","The album with the most tracks is titled \"Greatest Hits.\"\n"]}]},{"cell_type":"markdown","source":["## LangChain\n","- 자연어 처리(NLP)와 생성형 AI 응용 프로그램을 개발하기 위한 프레임워크입니다. 주로 대형 언어 모델(LLMs)과 같은 최신 NLP 기술을 기반으로 하여 다양한 작업을 자동화하거나 개선할 수 있는 도구와 서비스를 제공합니다.\n","- 이 프레임워크는 특히 언어 모델의 기능을 확장하고 이를 보다 쉽게 사용할 수 있도록 설계되었습니다.\n","- LangChain의 핵심 목표는 언어 모델을 활용하여 여러 복잡한 작업을 수행할 수 있도록 돕는 것이며, 특히 긴 텍스트, 문서 체인 또는 여러 단계의 워크플로를 필요로 하는 복잡한 응용 프로그램에 유용합니다.\n","\n","### LangChain의 구성 요소:\n","- Language Models: 언어 모델 자체를 사용하여 텍스트 생성, 요약, 번역 등의 작업을 수행합니다.\n","- Chains: 여러 모델 호출을 연결하여 복잡한 작업을 수행하는 논리적 흐름을 정의할 수 있습니다. 예를 들어, 텍스트를 요약한 후 요약된 텍스트에 대한 질의응답을 수행하는 체인을 만들 수 있습니다.\n","- Agents: 주어진 작업에 따라 다양한 툴을 선택하고 사용할 수 있는 지능형 에이전트입니다. 예를 들어, 정보 검색, API 호출 등을 수행하는 역할을 합니다.\n","- Memory: 이전의 상호작용 또는 맥락을 기억하는 기능입니다. 이를 통해 대화형 AI나 컨텍스트를 유지해야 하는 애플리케이션을 구현할 수 있습니다.\n","\n","### 활용 사례:\n","- 대화형 에이전트: 사용자와의 대화에서 맥락을 유지하며 대답할 수 있는 챗봇 개발.\n","문서 처리: 긴 문서나 여러 문서의 내용을 요약하거나 분석하는 애플리케이션.\n","- 지식 탐색: 사용자가 특정 주제에 대해 질문하면, 관련된 정보를 검색하고 이를 바탕으로 대답을 제공하는 시스템.\n","\n","### 통합:\n","- LangChain은 다양한 데이터 소스, API 및 언어 모델과 통합될 수 있으며, 이를 통해 다양한 도메인에서 사용될 수 있습니다. 예를 들어, SQL 데이터베이스에 쿼리를 보내고 결과를 요약하거나, 웹에서 정보를 수집한 후 이를 기반으로 질문에 답하는 작업을 수행할 수 있습니다.\n","\n","이 프레임워크는 특히 연구자, 데이터 사이언티스트, 개발자들이 생성형 AI를 활용하여 복잡한 텍스트 기반 응용 프로그램을 구축하는 데 큰 도움이 됩니다. LangChain은 이러한 작업을 더 쉽게, 더 직관적으로 구현할 수 있게 도와줍니다."],"metadata":{"id":"A-Z1kBU4qRjs"}},{"cell_type":"markdown","source":["https://www.langchain.com/\n","\n","https://www.langchain.com/langsmith\n"],"metadata":{"id":"jR2tQUaPs5Uo"}},{"cell_type":"markdown","source":["LangChain 패키지를 사용하여 RAG 시스템을 구성하는 예제\n","\n","주어진 질문에 대해 텍스트 파일에 저장된 문서에서 고나련 정보를 검색하고, 이를 기반으로 GPT 모델이 답변을 생성하는 것입니다."],"metadata":{"id":"p4yPwk2SvGSy"}},{"cell_type":"markdown","source":["- langchain-openai: LangChain과 OpenAI를 연결하는 패키지.\n","- faiss-cpu: 벡터 검색을 위한 FAISS 라이브러리의 CPU 버전.\n","- langchain_community: LangChain의 커뮤니티에서 개발된 추가 도구.\n","- tiktoken: OpenAI 모델에서 사용하는 토큰화 도구."],"metadata":{"id":"2dkkSH6EvRBQ"}},{"cell_type":"code","source":["!pip install -q openai\n","!pip install langchain langchain-openai -q\n","!pip install faiss-cpu -q\n","!pip install langchain_community -q\n","!pip install tiktoken -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6x8zUgctPwS","executionInfo":{"status":"ok","timestamp":1724113359168,"user_tz":-540,"elapsed":47723,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"12019feb-9a1b-4f26-a8c2-9ad7090161ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.4/362.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m509.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.5/391.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from openai import OpenAI\n","\n","## Set the API key and model name\n","MODEL=\"gpt-4o-mini-2024-07-18\"\n","client=OpenAI(api_key=\"\")"],"metadata":{"id":"KA0n9Fvtvo-n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["documents.txt"],"metadata":{"id":"Z1VYhrmDwHCf"}},{"cell_type":"code","source":["# Define the content for both basic and advanced selections\n","content = \"\"\"\n","Document 1:\n","Polar bears are directly impacted by climate change due to their dependence on sea ice for hunting seals. As global temperatures rise, sea ice is melting earlier and forming later each year, reducing the time polar bears have to hunt. This has led to decreased body condition, lower cub survival rates, and in some cases, increased mortality.\n","\n","Document 2:\n","The loss of sea ice habitat is one of the most significant threats to polar bear populations. As the ice retreats, polar bears are forced to travel greater distances to find food, leading to increased energy expenditure. This can result in malnutrition and a decline in reproductive success, further endangering the species.\n","\n","Document 3:\n","Climate change is not only affecting polar bears' hunting grounds but also their denning areas. Warmer temperatures and unstable snow conditions are making it more difficult for pregnant females to find suitable denning sites, which is crucial for the birth and survival of their cubs. This adds an additional layer of risk to polar bear populations already struggling due to loss of sea ice.\n","\n","Document 4:\n","As their traditional food sources become less accessible, some polar bears have been observed scavenging on human waste or preying on seabirds and their eggs. While this behavior may provide some short-term relief, it does not replace the high-fat diet they obtain from seals, which is essential for their long-term survival in the harsh Arctic environment.\n","\n","Document 5:\n","Recent studies suggest that if current trends in greenhouse gas emissions continue, polar bears could face extinction within this century. Conservation efforts are focusing on reducing global emissions and protecting critical polar bear habitats, but these efforts may not be sufficient if the climate continues to warm at the current rate.\n","\"\"\"\n","\n","# Write the content to a file\n","with open('documents.txt', 'w') as file:\n","    file.write(content)"],"metadata":{"id":"QVepbxgVwJHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1C__S68pKujK","executionInfo":{"status":"ok","timestamp":1724113419707,"user_tz":-540,"elapsed":337,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"a788b468-1b51-4674-a799-35798463717f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["documents.txt  sample_data\n"]}]},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain.vectorstores import FAISS\n","from langchain.document_loaders import TextLoader\n","from langchain.chains import RetrievalQA"],"metadata":{"id":"79y2zwzTKxYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- OpenAIEmbeddings: 문서를 벡터로 임베딩하는 클래스입니다. 각 문서를 숫자 벡터로 변환하여 유사성 검색을 할 수 있게 만듭니다.\n","- ChatOpenAI: OpenAI의 GPT 모델을 사용하는 클래스입니다. model을 지정하여 특정 GPT 모델을 사용할 수 있으며, fine-tuned된 모델도 지원합니다.\n","- FAISS (Facebook AI Similarity Search): 벡터를 빠르게 검색하기 위한 라이브러리입니다. Facebook에서 개발한 AI 유사성 검색 라이브러리로, 임베딩된 문서들 중에서 가장 관련성이 높은 문서를 빠르게 검색할 수 있습니다.\n","- TextLoader: 문서를 텍스트 형식으로 불러오는 클래스입니다. 이 클래스는 주어진 파일에서 텍스트 데이터를 로드할 때 사용됩니다.\n","- RetrievalQA: 검색된 문서와 GPT 모델을 결합하여 질의응답 작업을 수행하는 체인입니다."],"metadata":{"id":"KR_JjRm9K_Oi"}},{"cell_type":"code","source":["# OpenAI API 키 설정\n","import os\n","os.environ['OPENAI_API_KEY']='' # OPEN AI APi 키 입력\n","\n","# 1. 문서 로드\n","# 텍스트 파일을 사용해 문자를 로드합니다\n","loader=TextLoader('documents.txt')\n","documents=loader.load()"],"metadata":{"id":"CdjXo3aSM13i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 코드는 문서를 로드하는 부분입니다.\n","\n","- **TextLoader**를 사용하여 documents.txt 파일을 불러옵니다. 이 파일은 시스템에서 검색할 수 있는 문서들이 포함된 파일입니다.\n","- **loader.load()**를 호출하여 문서를 읽어들이고, 이를 documents라는 변수에 저장합니다. 이 변수는 나중에 벡터화(임베딩)할 때 사용됩니다."],"metadata":{"id":"PT7qVaDuNlOo"}},{"cell_type":"code","source":["# 2. 문서 임베딩 및 인덱싱\n","# 문서를 벡터로 변환한 후, FAISS 벡터 저장소에 자정합니다\n","embeddings = OpenAIEmbeddings()\n","vector_store=FAISS.from_documents(documents, embeddings)"],"metadata":{"id":"a-u10ymMNzxv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 부분은 문서를 벡터로 변환하고 이를 인덱싱하는 단계입니다.\n","\n","- OpenAIEmbeddings: 문서를 숫자 벡터로 변환하는데 사용됩니다. 임베딩은 텍스트의 의미를 벡터 공간에서 표현하는 방법으로, 유사성 검색에 중요한 역할을 합니다.\n","- FAISS.from_documents: documents 변수를 임베딩한 후, FAISS 인덱스를 생성합니다. 이 인덱스는 문서를 벡터로 변환한 후 빠르게 유사성을 계산하고 관련 문서를 검색하는 데 사용됩니다."],"metadata":{"id":"dawCuzdsOTK2"}},{"cell_type":"code","source":["# 3. Fine-tuned GPT 모델을 사용한 LLM 생성\n","# Fine-tuned된 GPT 모델을 사용할 수 있도록 설정합니다.\n","llm=ChatOpenAI(model='gpt-4o-mini-2024-07-18', temperature=0.3)"],"metadata":{"id":"5v4FQj2XPDbT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 부분은 GPT 모델을 설정하는 부분입니다.\n","\n","- ChatOpenAI: OpenAI의 GPT 모델을 사용합니다. 여기서 model 파라미터로 Fine-tuned된 GPT 모델을 지정하고 있습니다. \"gpt-4o-mini-2024-07-18\"은 Fine-tuning된 GPT 모델의 ID입니다.\n","- temperature: 텍스트 생성을 제어하는 파라미터로, 값이 낮을수록 더 결정적인(deterministic) 결과를 생성하고, 값이 높을수록 생성되는 텍스트의 창의성(랜덤성)이 높아집니다."],"metadata":{"id":"ssD2L9NgPdPb"}},{"cell_type":"code","source":["# 4. 문서 검색 개수 조정 (최대 5개의 문서 검색)\n","# 사용자가 입력한 쿼리와 가장 유사한 5개의 결과를 검색하여 반환하겠다는 의미\n","retriever = vector_store.as_retriever(search_kwargs={'k':5})"],"metadata":{"id":"ethZ_VACPzaL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 부분에서는 검색할 문서의 개수를 설정합니다.\n","\n","- retriever: vector_store에서 as_retriever 메서드를 사용하여 문서 검색을 위한 리트리버(retriever)를 생성합니다.\n","- k 값: search_kwargs에서 k를 5로 설정하여, 사용자가 질문을 할 때 최대 5개의 문서가 검색되도록 설정합니다. 즉, 질문과 관련된 상위 5개의 문서를 검색하여 답변에 사용할 수 있도록 합니다."],"metadata":{"id":"DM8SkD0lPw0Q"}},{"cell_type":"code","source":["# 5. RAG 구현\n","# RetrievalQA 체인을 사용해 검색된 문서를 기반으로 LLM을 통해 답변을 생성합니다.\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=llm, # Fine-tuned된 GPT 모델\n","    chain_type=\"stuff\", # 'stuff' 체인을 사용하여 검색된 문서들을 단일 입력으로 결합하여 처리.\n","    retriever=retriever, # 검색할 문서의 수를 5개로 설정한 retriever\n","    return_source_documents=True # 검색된 문서를 포함하여 반환할지 여부를 지정\n",")"],"metadata":{"id":"tkwWC3Y-QGOE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 부분은 Retrieval-Augmented Generation (RAG) 체인을 구현하는 부분입니다.\n","\n","- RetrievalQA: RetrievalQA 체인은 검색된 문서를 GPT 모델과 결합하여 질문에 대한 답변을 생성합니다.\n","- from_chain_type: 여기서는 \"stuff\" 체인 유형을 사용합니다. 이는 검색된 문서들을 모두 결합하여 하나의 텍스트로 처리하는 방식입니다.\n","  다른 체인 방식으로는 map-reduce나 refine 등의 방법이 있습니다.\n","  - Map-reduce: 문서를 나눠서 처리하고, 나중에 결합하는 방식.\n","  - Refine: 문서를 한 번에 하나씩 처리하고, 그 결과를 점진적으로 개선하는 방식.\n","- retriever: 이 retriever는 벡터 스토어(예: FAISS)에서 관련 문서를 검색한 후, 해당 문서들을 LLM에게 제공하여 답변을 생성하도록 돕습니다.\n","- return_source_documents=True: 검색된 문서를 응답에 포함하여 반환할지 여부를 지정합니다. True로 설정하면, 검색된 문서들을 답변과 함께 반환합니다. 이렇게 하면 사용자가 생성된 답변 외에도 검색된 문서들을 검토할 수 있습니다."],"metadata":{"id":"RN3D02wCRKz1"}},{"cell_type":"code","source":["# 6. 질문에 대한 답변 생성\n","query='What is the impact of climate change on polar bears?'\n","result=qa_chain.invoke({'query': query}) # invoke 메서드를 사용하여 query 전달\n","\n","# 7. 결과 출력\n","print(result['result'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SMAh4mL2UZul","executionInfo":{"status":"ok","timestamp":1724116100551,"user_tz":-540,"elapsed":3024,"user":{"displayName":"Myoungshin Lee","userId":"12933823317741169406"}},"outputId":"1561ba1b-0456-4571-f4a3-dd236708b518"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Climate change has a significant impact on polar bears primarily due to their dependence on sea ice for hunting seals. As global temperatures rise, sea ice is melting earlier and forming later each year, which reduces the time polar bears have to hunt. This has led to decreased body condition, lower cub survival rates, and increased mortality in some cases.\n","\n","The loss of sea ice habitat forces polar bears to travel greater distances to find food, resulting in increased energy expenditure, malnutrition, and a decline in reproductive success. Additionally, warmer temperatures and unstable snow conditions make it more difficult for pregnant females to find suitable denning sites, which is crucial for the birth and survival of their cubs.\n","\n","As traditional food sources become less accessible, some polar bears have started scavenging on human waste or preying on seabirds and their eggs, but this behavior does not provide the high-fat diet they need for long-term survival. If current trends in greenhouse gas emissions continue, polar bears could face extinction within this century. Conservation efforts are being made to reduce emissions and protect their habitats, but these may not be sufficient if climate change continues at the current rate.\n"]}]}]}